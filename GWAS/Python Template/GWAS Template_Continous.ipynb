{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSLS GWAS Template: Continuous, Quantitative Outcome\n",
    "\n",
    "- This template goes through the necessary joining, cleaning, and tests to run a simplified GWAS with a continuous, quantitative outcome. (preset to look at chromosome 1). \n",
    "- Throughout the template, there are small sections that will require input from you, the user. \n",
    "- **Please go through the template and fill out these required sections before running the cells.** \n",
    "- **They will be prefaced with the following comment:**\n",
    "\n",
    "`####################################### PLEASE EDIT THIS SECTION #######################################`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages/Paths\n",
    "\n",
    "- This section is for importing the `hail`, `pandas`, and `os` packages.\n",
    "- It is also saving the Google storage paths to the Workspace Bucket (`bucket`) and the WGS Exome HailMatrix Table (`exome_split_mt_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import os\n",
    "import pandas as pd\n",
    "from hail.plot import show\n",
    "from bokeh.plotting import output_file, save\n",
    "import bokeh.io\n",
    "from bokeh.io import *\n",
    "from bokeh.resources import INLINE\n",
    "bokeh.io.output_notebook(INLINE) \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the workspace bucket path\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# this saves the path as a variable\n",
    "exome_split_mt_path = os.getenv(\"WGS_EXOME_SPLIT_HAIL_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET HG38 AS DEFAULT REF.\n",
    "hl.init(default_reference=\"GRCh38\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Phenotype Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, the template is **importing** your saved .tsv file as a **Hail Table**. \n",
    "- It must be a tab-separated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### PLEASE EDIT THIS SECTION #######################################\n",
    "file_name = \"file_name_here.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy tsv file from the bucket to the current working space\n",
    "os.system(f\"gsutil cp '{bucket}/data/{file_name}' .\")\n",
    "pheno = f'{bucket}/data/{file_name}'\n",
    "pheno_df = (hl.import_table(pheno,\n",
    "                              types={'person_id':hl.tstr},\n",
    "                              impute=True,\n",
    "                              key='person_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Genotype Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <em>AllofUs</em> provides short variants for all the 245,394 short read WGS participants that are stored in the format of a [Hail VDS](https://support.researchallofus.org/hc/en-us/articles/14927774297620-The-new-VariantDataset-VDS-format-for-All-of-Us-short-read-WGS-data).\n",
    "\n",
    "- This template uses the **Exome subset with split rows for alleles**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the path in as a HailMatrix Table\n",
    "exome_split_mt = hl.read_matrix_table(exome_split_mt_path)\n",
    "\n",
    "# check the dimensions of the HailMatrix Table\n",
    "exome_split_mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Specific Regions of the Genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Filter for genome region you interested in**. It is preset to chromosome 1. \n",
    "- Note: If you would like to run a full GWAS, comment this section out. Be careful, though, it is costly and time consuming to run a full GWAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### PLEASE EDIT THIS SECTION [OPTIONAL] #######################################\n",
    "# this is the chromosome interval to extract this variant\n",
    "test_intervals = ['chr1']\n",
    "\n",
    "# the function to filter variant region\n",
    "exome_split_mt = hl.filter_intervals(\n",
    "    exome_split_mt,\n",
    "    [hl.parse_locus_interval(x,)\n",
    "     for x in test_intervals])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Genotype and Phenotype Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must **merge** all of the genotype and phenotype data into a **final HailMatrix Table**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this merges the phenotype data with the genotype data\n",
    "cohort_mt = exome_split_mt.semi_join_cols(pheno_df)\n",
    "cohort_mt = cohort_mt.annotate_cols(pheno = pheno_df[cohort_mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <em>AllofUs</em> provides tables with additional information on those in the cohort, depending on what type of genomic data you are using.\n",
    "- We will be using two of these tables which give **ancestry** and **relatedness** information on those in our cohort.\n",
    "- There is [more information on auxiliary tables](https://support.researchallofus.org/hc/en-us/articles/4614687617556-How-the-All-of-Us-Genomic-data-are-organized) provided by AoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the path string that loads auxiliary tables\n",
    "auxiliary_path = \"gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancestry Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ancestry path inside auxiliary path\n",
    "ancestry_path = f'{auxiliary_path}/ancestry'\n",
    "\n",
    "# save the ancestry .tsv file path as a variable\n",
    "ancestry_pred_path = f'{ancestry_path}/ancestry_preds.tsv'\n",
    "\n",
    "# import as a Hail Table\n",
    "ancestry_pred = hl.import_table(ancestry_pred_path,\n",
    "                               key=\"research_id\", \n",
    "                               impute=True, \n",
    "                               types={\"research_id\":\"tstr\",\"pca_features\":hl.tarray(hl.tfloat)})\n",
    "\n",
    "# annotates each person id with their ancestry information\n",
    "cohort_mt = cohort_mt.annotate_cols(anc = ancestry_pred[cohort_mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relatedness Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import a list of **related samples** to remove from `cohort_mt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relatedness path inside auxiliary path\n",
    "relatedness = f'{auxiliary_path}/relatedness'\n",
    "\n",
    "# save the relatedness \n",
    "related_samples_path = f'{relatedness}/relatedness_flagged_samples.tsv'\n",
    "\n",
    "# import as Hail Table\n",
    "related_remove = hl.import_table(related_samples_path,\n",
    "                                 types={\"sample_id\":\"tstr\"},\n",
    "                                key=\"sample_id\")\n",
    "\n",
    "# removes sample ids that are related (kinship score > 0.1)\n",
    "final_mt = cohort_mt.anti_join_cols(related_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check difference in sample size\n",
    "# print(cohort_mt.count())\n",
    "# print(final_mt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stratify the linear regression by choosing your stratification variable `strat_variable` and the group you are interested in within the stratification variable `strat_type`. \n",
    "\n",
    "- An example would be `strat_variable = final_mt.pheno.sex_at_birth` and `strat_type = \"Male\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### PLEASE EDIT THIS SECTION [OPTIONAL] #######################################\n",
    "strat_variable = insert_strat_variable\n",
    "strat_type = \"insert_strat_type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You must uncomment the cell below (delete the `#`) for the stratification to run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_mt = final_mt.filter_cols(strat_variable == strat_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will check:\n",
    "- **minor allele frequency (MAF)**\n",
    "- **Hardyâ€“Weinberg equilibrium (HWE)**\n",
    "\n",
    "As a note: The genomic data is preprocessed for **sex discrepancy** and **heterozygosity rate (AB >= 0.2)** by AoU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAF\n",
    "\n",
    "- MAF thresholds should be sample size-dependent; typically 0.01 for large (N=100,000) and 0.05 for moderate (N=10,000) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### PLEASE EDIT THIS SECTION [OPTIONAL] #######################################\n",
    "MAF_threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mt = hl.variant_qc(final_mt)\n",
    "final_mt = final_mt.filter_rows(hl.min(final_mt.variant_qc.AF) > MAF_threshold, keep = True)\n",
    "#final_mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardy-Weinberg Equilibrium\n",
    "\n",
    "- In this section, we are making sure that our variants meet Hardy-Weinberg equilibrium. \n",
    "- It is preset to a p-value of `1e-6` to check genotyping quality, but can be adjusted depending on sample size and type of analysis.\n",
    "- I am using the HWE threshold suggested [in this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### PLEASE EDIT THIS SECTION [OPTIONAL] #######################################\n",
    "hwe_threshold = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mt = final_mt.filter_rows(final_mt.variant_qc.p_value_hwe > hwe_threshold, keep = True)\n",
    "#final_mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will run a **linear regression**:\n",
    "- a **numerical outcome** of your choice\n",
    "- number of alternate alleles as the **predictor** (Additive Model)\n",
    "- **covariates** adjusted for in the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at variables in HailMatrix Table\n",
    "final_mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### PLEASE EDIT THIS SECTION #######################################\n",
    "outcome = insert_outcome_here\n",
    "covar = [insert_covariates_here] # separate items with commas \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unadjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the regression **without adjustment** for covariates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unadj_reg = hl.linear_regression_rows(\n",
    "    y = outcome,\n",
    "    x = final_mt.GT.n_alt_alleles(),\n",
    "    covariates = [1.0]\n",
    ")\n",
    "\n",
    "unadj_reg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the regression **with adjustment** for covariates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_reg = hl.linear_regression_rows(\n",
    "    y = outcome,\n",
    "    x = final_mt.GT.n_alt_alleles(),\n",
    "    covariates = [1.0] + covar\n",
    ")\n",
    "\n",
    "adj_reg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results\n",
    "The code below will save your adjusted regression results. The current file name is `adj_reg`. You can adjust the file name by using the `file_name` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"adj_reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_reg_flat = adj_reg.flatten()\n",
    "\n",
    "adj_reg_save_path = f'{bucket}/data/{file_name}.tsv.bgz'\n",
    "\n",
    "adj_reg_flat.export(adj_reg_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below to load adjusted regression results back into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_reg_results = hl.import_table(adj_reg_save_path, \n",
    "#                                   types={\"locus\": hl.tstr, \"alleles\": hl.tarray(hl.tstr), \n",
    "#                                          \"beta\": hl.tfloat64, \"p_value\": hl.tfloat64, \n",
    "#                                          \"fit.n_iterations\": hl.tint32})\n",
    "\n",
    "# adj_reg_results = adj_reg_results.annotate(locus=hl.parse_locus(adj_reg.locus, reference_genome='GRCh38'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Plot - Adjusted Regression\n",
    "\n",
    "- This section creates a manhatten plot of the adjusted regression results. It then saves the plot to your workspace under `\"manhattan.html\"`.\n",
    "- Uncomment `show_bokeh(p)` to view the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bokeh(plot_fig):\n",
    "    try:\n",
    "        bokeh.plotting.reset_output()\n",
    "        bokeh.plotting.output_notebook()\n",
    "        bokeh.plotting.show(plot_fig)\n",
    "    except:\n",
    "        bokeh.plotting.output_notebook()\n",
    "        bokeh.plotting.show(plot_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.manhattan(adj_reg.p_value)\n",
    "#show_bokeh(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(\"manhattan.html\")\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy saved bokeh plot to the workspace bucket.\n",
    "!gsutil cp manhattan.html {bucket}/data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
